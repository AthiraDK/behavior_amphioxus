{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78742d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb49b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba85644",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e913eb2",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "Information like age of the animal, drugs admiistered, details of light-stimuli used etc are stored in a pickle file which was created using ```eda_metadata.ipynb```.\n",
    "We can load that data and use these information to select subsets of data for our analysis. \n",
    "This would allow us to compare the use of clusters across drugs,light-stimuli and age factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_pickle('../../data/amphioxus_metadata_final500.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a filename column to match with the filename column in the dataset\n",
    "df_meta['filename'] = df_meta['filename_video'].apply(lambda x: x.split('.avi')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf1ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.light.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cdb1e",
   "metadata": {},
   "source": [
    "# Loading data : featureset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('../../results/featureset_v4_29072023.h5', key='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e7f54",
   "metadata": {},
   "source": [
    "### Thresholding ```speed_MOUTH```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d4ab5",
   "metadata": {},
   "source": [
    "This is should ideally be done while creating the feature dataset. \n",
    "Since the ```speed_MOUTH``` feature has not been previously processed for outlier detection, I am doing it here using simple thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8879cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['speed_MOUTH'] < 50]['speed_MOUTH'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1022bc9",
   "metadata": {},
   "source": [
    "- From the histogram it seems like **20** is a reasonable threshold.\n",
    "- Also since **NaN values** of ```speed_MOUTH``` can be meaningful, cannot remove the rows where ```df[speed_MOUTH].isna()``` is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['speed_MOUTH'].isna())|(df['speed_MOUTH'] < 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97754df",
   "metadata": {},
   "source": [
    "## Get features from wildtype (control) animals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df.merge(df_meta, how='left', on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe74bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control = df_merged[(df_merged['age'] > 50)&(df_merged['drugs']=='none')&(df_merged['light']=='None')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35251bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of control/ wildtype videos :{len(df_control.filename.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e8ef1",
   "metadata": {},
   "source": [
    "## Preprocessing : Normalizing or Scaling the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbe953",
   "metadata": {},
   "source": [
    "Since each of the biophysical measures have different value ranges and scales, I think it would be better to normalize them so that each of the feature has same weightage when we perform dimensionality reduction and clustering. \n",
    "- This is frequently done before PCA\n",
    "- Is it required in the case of UMAP ?\n",
    "\n",
    "What can be done is a comparison of the UMAP results with and without scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df.copy() #deep copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8a2b0",
   "metadata": {},
   "source": [
    "### Curvatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1865f",
   "metadata": {},
   "source": [
    "Since curvatures can be positive and negative, it might be better to use a scaler in [-1, 1] range, so that -ve & +ve curvatures are better distinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_curv = list(df_prep.filter(like='curv').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep[cols_curv] = MinMaxScaler(feature_range=(-1,1)).fit_transform(df_prep[cols_curv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72924a95",
   "metadata": {},
   "source": [
    "### Speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c682999",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_speed = list(df_prep.filter(like='speed').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a74d8c",
   "metadata": {},
   "source": [
    "Since speeds cannot have negative value, using a MinMaxScaler in [0, 1] (default) range should be good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ec0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep[cols_speed] = MinMaxScaler().fit_transform(df_prep[cols_speed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bec3b",
   "metadata": {},
   "source": [
    "### Quirkiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514dbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep['quirkiness'] = pd.DataFrame(MinMaxScaler().fit_transform(df_prep['quirkiness'].values.reshape(-1,1)), index=df_prep.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d636f",
   "metadata": {},
   "source": [
    "### Preprocessed control dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_merged = df_prep.merge(df_meta, how='left', on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_control = df_prep_merged[(df_prep_merged['age'] > 50)&(df_prep_merged['drugs']=='none')&(df_prep_merged['light']=='None')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of control/ wildtype videos :{len(df_prep_control.filename.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d79ec",
   "metadata": {},
   "source": [
    "# Selecting features to be used for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control = pd.read_hdf('../../results/featureset_used_for_UMAPclustering_18072023.h5', key='features_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_speed = list(df_control.filter(like='speed').columns)\n",
    "feats_to_use = cols_speed #+ cols_curv + ['quirkiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = df_control[feats_to_use]\n",
    "# df_in_prep = df_prep_control[feats_to_use]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3d3b5",
   "metadata": {},
   "source": [
    "# NaN masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ac7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = df_in.fillna(value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31196169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_in_prep = df_in_prep.fillna(value=-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ebf41",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(min_dist=0.0, n_jobs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5d537",
   "metadata": {},
   "source": [
    "## Without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b67624",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.fit_transform(df_in.values)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'../../results/umap_model_29072023_v1.joblib'\n",
    "joblib.dump(reducer, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd443a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## With preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d542f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reducer_prep = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a7b53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding_prep = reducer_prep.fit_transform(df_in_prep.values)\n",
    "embedding_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdde00c",
   "metadata": {},
   "source": [
    "## Comparing how UMAP looks when features are scaled and when features aren't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9388dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "# axes[1].scatter(embedding_prep[:, 0],embedding_prep[:, 1], s=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(15,15), sharex=True)\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "\n",
    "hue_feats = {'mouth': df_in['speed_MOUTH'],\n",
    "             'quirkiness': df_control['quirkiness'],\n",
    "             'mean_dorsal_speeds': df_in.filter(like='speed_D').mean(axis=1),\n",
    "             'mean_ventral_speeds': df_in.filter(like='speed_V').mean(axis=1),\n",
    "             'mean_speeds': df_in.filter(like='speed_').mean(axis=1),\n",
    "#              'length': df_control['len_sum_of_parts'],\n",
    "             'mean_curv': df_control.filter(like='curv').abs().mean(axis=1),\n",
    "             'speed_NT': df_in['speed_NT'],\n",
    "            }\n",
    "\n",
    "for i, key_hue in enumerate(hue_feats.keys()):\n",
    "    axes[i+1].scatter(embedding[:, 0],embedding[:, 1], c= hue_feats[key_hue], s=0.2)\n",
    "    axes[i+1].set_title(key_hue)\n",
    "for ax in axes:  \n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "# fig.savefig('../../results/umap_comparisons/umap_raw_feat_distributions_v3_29072023.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control['umap_raw_0'] = embedding[:,0]\n",
    "df_control['umap_raw_1'] = embedding[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_control.filter(like='umap').to_hdf('../../results/UMAPclustering_29072023_1501.h5', key='features_with_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335bb09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(3,3, figsize=(15,15))\n",
    "# axes= axes.ravel()\n",
    "# axes[0].scatter(embedding_prep[:, 0],embedding_prep[:, 1], s=0.2)\n",
    "\n",
    "# hue_feats = {'mouth': df_in_prep['speed_MOUTH'],\n",
    "#              'quirkiness': df_in_prep['quirkiness'],\n",
    "#              'mean_dorsal_speeds': df_in_prep.filter(like='speed_D').mean(axis=1),\n",
    "#              'mean_ventral_speeds': df_in_prep.filter(like='speed_V').mean(axis=1),\n",
    "#              'mean_speeds': df_in_prep.filter(like='speed_').mean(axis=1),\n",
    "# #              'length': df_in_prep['len_sum_of_parts'],\n",
    "#              'mean_curv': df_in_prep.filter(like='curv').abs().mean(axis=1),\n",
    "#              'speed_NT': df_in_prep['speed_NT'],\n",
    "#             }\n",
    "\n",
    "# for i, key_hue in enumerate(hue_feats.keys()):\n",
    "#     axes[i+1].scatter(embedding_prep[:, 0],embedding_prep[:, 1], c= hue_feats[key_hue], s=0.2)\n",
    "#     axes[i+1].set_title(key_hue)\n",
    "# for ax in axes:  \n",
    "#     ax.set_aspect('equal', 'datalim')\n",
    "# fig.savefig('../results/umap_comparisons/umap_prep_feat_distributions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a32ddf",
   "metadata": {},
   "source": [
    "Seems like the UMAP space looks good without preprocessing. \n",
    "- TODO: \n",
    "    - Think of how preprocessing affects the distances\n",
    "    - Also, need to check how UMAP space changes according to min_dist, n_neigbors and metric arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bb802",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## UMAP parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e82512",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reducer_maha = umap.UMAP(metric= 'mahalanobis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688ff0e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding_maha = reducer_maha.fit_transform(df_in.values)\n",
    "embedding_maha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8874fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "axes[1].scatter(embedding_maha[:, 0],embedding_maha[:, 1], s=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6b404",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(15,15))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding_maha[:, 0],embedding_maha[:, 1], s=0.2)\n",
    "\n",
    "hue_feats = {'mouth': df_in['speed_MOUTH'],\n",
    "             'quirkiness': df_in['quirkiness'],\n",
    "             'mean_dorsal_speeds': df_in.filter(like='speed_D').mean(axis=1),\n",
    "             'mean_ventral_speeds': df_in.filter(like='speed_V').mean(axis=1),\n",
    "             'mean_speeds': df_in.filter(like='speed_').mean(axis=1),\n",
    "#              'length': df_in['len_sum_of_parts'],\n",
    "             'mean_curv': df_in.filter(like='curv').abs().mean(axis=1),\n",
    "             'speed_NT': df_in['speed_NT'],\n",
    "            }\n",
    "\n",
    "for i, key_hue in enumerate(hue_feats.keys()):\n",
    "    axes[i+1].scatter(embedding_maha[:, 0],embedding_maha[:, 1], c= hue_feats[key_hue], s=0.2)\n",
    "    axes[i+1].set_title(key_hue)\n",
    "for ax in axes:  \n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "fig.savefig('../../results/umap_comparisons/umap_maha_feat_distributions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095a634",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uplaod UMAP results from saved data\n",
    "# df_results_umap = pd.read_hdf('../../results/UMAPclustering_18072023_2118.h5', key='features_with_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ced23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = df_results_umap.values\n",
    "# embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(15,15))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "\n",
    "hue_feats = {'mouth': df_control['speed_MOUTH'],\n",
    "             'quirkiness': df_control['quirkiness'],\n",
    "             'mean_dorsal_speeds': df_control.filter(like='speed_D').mean(axis=1),\n",
    "             'mean_ventral_speeds': df_control.filter(like='speed_V').mean(axis=1),\n",
    "             'mean_speeds': df_control.filter(like='speed_').mean(axis=1),\n",
    "#              'length': df_control['len_sum_of_parts'],\n",
    "             'mean_curv': df_control.filter(like='curv').abs().mean(axis=1),\n",
    "             'speed_NT': df_control['speed_NT'],\n",
    "            }\n",
    "\n",
    "for i, key_hue in enumerate(hue_feats.keys()):\n",
    "    axes[i+1].scatter(embedding[:, 0],embedding[:, 1], c= hue_feats[key_hue], s=0.2)\n",
    "    axes[i+1].set_title(key_hue)\n",
    "for ax in axes:  \n",
    "    ax.set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a82ae",
   "metadata": {},
   "source": [
    "## Cluster based on UMAP only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5975f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_samples= 1, #larger values implies more points considered as noise\n",
    "    min_cluster_size= 20000, #smallest size grouping to be considered as a cluster\n",
    "    cluster_selection_epsilon=1,\n",
    "    prediction_data=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterer.fit_predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bf77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc580901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clusters = {f'cluster_{i}':np.sum(labels==i) for i in list(np.unique(labels))}\n",
    "dict_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_pal = sns.color_palette('tab10', 10)\n",
    "c_dict = {i: c_pal[i+1] for i in np.unique(labels)}\n",
    "labels_c = [c_dict[lab] for lab in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "axes[1].scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1], c=labels_c, s=1)\n",
    "\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in c_dict.values()]\n",
    "plt.legend(markers, c_dict.keys(), numpoints=1)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(clusterer, f'../../results/hdbscan_model_29072023_v1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296527a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('../../results/umap_hdbscan_results_19072023_1156.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f15f91",
   "metadata": {},
   "source": [
    "## Cluster using UMAP and other feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_plus = np.hstack([embedding, df_control.filter(like='curv').abs().mean(axis=1).values.reshape(-1,1), df_in['speed_MOUTH'].values.reshape(-1,1)])\n",
    "embedding_plus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e752eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_plus = hdbscan.HDBSCAN(\n",
    "    min_samples= 1, #larger values implies more points considered as noise\n",
    "    min_cluster_size= 20000, #smallest size grouping to be considered as a cluster\n",
    "    prediction_data=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ccdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_plus = clusterer_plus.fit_predict(embedding_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clusters_plus = {f'cluster_{i}':np.sum(labels_plus==i) for i in list(np.unique(labels_plus))}\n",
    "dict_clusters_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c60cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dict_plus = {i: c_pal[i+1] for i in np.unique(labels_plus)}\n",
    "labels_plus_c = [c_dict_plus[lab] for lab in labels_plus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "axes[1].scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1], c=labels_plus_c, s=1)\n",
    "\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in c_dict_plus.values()]\n",
    "plt.legend(markers, c_dict_plus.keys(), numpoints=1)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81721e",
   "metadata": {},
   "source": [
    "### test with ```cluster_selection_parameter =1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_plus_ = hdbscan.HDBSCAN(\n",
    "    min_samples= 1, #larger values implies more points considered as noise\n",
    "    min_cluster_size= 20000, #smallest size grouping to be considered as a cluster\n",
    "    cluster_selection_epsilon=1,\n",
    "    prediction_data=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0259d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_plus_ = clusterer_plus_.fit_predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clusters_plus_ = {f'cluster_{i}':np.sum(labels_plus_==i) for i in list(np.unique(labels_plus_))}\n",
    "dict_clusters_plus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dict_plus_ = {i: c_pal[i+1] for i in np.unique(labels_plus_)}\n",
    "labels_plus_c_ = [c_dict_plus_[lab] for lab in labels_plus_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "axes[1].scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1], c=labels_plus_c_, s=1)\n",
    "\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in c_dict_plus_.values()]\n",
    "plt.legend(markers, c_dict_plus_.keys(), numpoints=1)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7d938",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c783e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control['hdbscan'] = labels\n",
    "df_control['hdbscan_plus'] = labels_plus\n",
    "df_control['hdbscan_plus_'] = labels_plus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control.to_hdf('../../results/UMAP_HDBSCANclustering_29072023_1832.h5', key='features_with_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240aff8a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Try DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964b5e2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed71e22",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DBSCAN_cluster = DBSCAN(n_jobs=40).fit(embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7108f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels_db = DBSCAN_cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ddb96",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_clusters_db = {f'cluster_{i}':np.sum(labels_db==i) for i in list(np.unique(labels_db))}\n",
    "dict_clusters_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25361f39",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_pal_10 = sns.color_palette('tab10', 10)\n",
    "c_db_dict = {i: c_pal_10[i+1] for i in np.unique(labels_db)}\n",
    "labels_db_c = [c_db_dict[lab] for lab in labels_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b7146",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(7,7))\n",
    "axes= axes.ravel()\n",
    "\n",
    "axes[1].scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1], c=labels_db_c, s=1)\n",
    "\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in c_db_dict.values()]\n",
    "plt.legend(markers, c_db_dict.keys(), numpoints=1)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc25527",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_control['hdbscan_clusters'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4c6aa",
   "metadata": {},
   "source": [
    "# Checking clustering results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4bf6c",
   "metadata": {},
   "source": [
    "Does each of the cluster exist in more than 1 video? else it could be some outlier / noise.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control[['hdbscan_clusters', 'filename']].groupby('hdbscan_clusters').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9091a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_nonfeats = ['filename', 'frame',\n",
    "       'path_to_video', 'filename_video', 'date', 'time', 'light', 'drugs',\n",
    "       'duration', 'age', 'stim_on', 'stim_off', 'stim_RGB', 'dlc_result_file',\n",
    "       'umap_raw_0', 'umap_raw_1', 'hdbscan_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59258bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control.filter(items=cols_nonfeats).to_hdf('../../data/umap_hdbscan_control.h5', key='control_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f44bb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Randomly sample each cluster and examine the postures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa35e76",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cec57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d298498",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from dlc_helper import DLC_tracking\n",
    "from video_utils import get_rois, find_square_bounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e643d92",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widegts\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072185c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_results_control = pd.read_hdf('../../data/umap_hdbscan_control.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b84a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_fns = df_results_control.filename.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47be4e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_fns.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be08b42",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_results_control[df_results_control['filename']==sample_fns.iloc[0]].groupby('hdbscan_clusters').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d902f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_rois_per_video(path_to_video, path_to_dlc_coords, crop=True):\n",
    "    \n",
    "    \n",
    "\n",
    "    dlc_folder, dlc_filename = os.path.split(path_to_dlc_coords)\n",
    "    dlc_obj = DLC_tracking(dlc_filename, dlc_folder)\n",
    "    \n",
    "    dict_bbox = dlc_obj.find_bbox_dlc()\n",
    "    dict_rois = {}\n",
    "    \n",
    "    cap = cv2.VideoCapture(path_to_video)\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    count = 0\n",
    "    while count < total_frames:\n",
    "        _, image = cap.read()\n",
    "\n",
    "        count += 1\n",
    "        if crop:\n",
    "            # Find coords of the roi corners\n",
    "            x, y, w, h = dict_bbox[count-1]\n",
    "            mid_x = x + (w/2)\n",
    "            mid_y = y + (h/2)\n",
    "            try:\n",
    "                x = int(x)\n",
    "                y = int(y)\n",
    "                w = int(w)\n",
    "                h = int(h)\n",
    "                x_new, y_new, w_new, h_new = find_square_bounding(x,y,w,h, height_max = 150, width_max = 150)\n",
    "                roi_box_padded = image[y_new:y_new+h_new, x_new:x_new+w_new]\n",
    "                dict_rois[count-1] = roi_box_padded\n",
    "\n",
    "            except ValueError:\n",
    "                print(f'Value Error encountered for frame {count-1}')\n",
    "            except Exception as e:\n",
    "                print(f'Error encountered')\n",
    "                print(e)\n",
    "\n",
    "        else:\n",
    "            dict_rois[count-1] = image\n",
    "            \n",
    "#         cv2.waitKey(30)\n",
    "            \n",
    "    cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "        \n",
    "    return dict_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeea6b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = '20180724_102237_1_5m0s_None_None_None_INVERTED'\n",
    "df_filename = df_results_control[df_results_control['filename']== filename]\n",
    "clusters_fn = sorted(df_filename['hdbscan_clusters'].unique())\n",
    "path_to_video = df_filename['path_to_video'].unique()[0]\n",
    "path_to_dlc_coords = df_filename['dlc_result_file'].unique()[0]\n",
    "test_dict = get_rois_per_video(path_to_video, path_to_dlc_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a759ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mega_dict = {i:[] for i in [-1,0,1,2,3,4,5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb921dfa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grouped = df_filename.groupby('hdbscan_clusters')\n",
    "    \n",
    "for i, clus in (enumerate(clusters_fn)):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(5*8, 8))\n",
    "    axes = axes.ravel()\n",
    "    df = df_grouped.get_group(clus)\n",
    "    if len(df.index) > 5:\n",
    "        df_samples = df.sample(5)\n",
    "    else:\n",
    "        df_samples = df\n",
    "\n",
    "    frames = list(df_samples['frame'])\n",
    "\n",
    "    for j, f in enumerate(frames):\n",
    "        axes[j].imshow(test_dict[f])\n",
    "        axes[j].set_title(f)\n",
    "        \n",
    "        if j == 0:\n",
    "            axes[j].set_ylabel(f'cluster: {clus}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd113f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for filename in sample_fns:\n",
    "    df_filename = df_results_control[df_results_control['filename']== filename]\n",
    "    clusters_fn = sorted(df_filename['hdbscan_clusters'].unique())\n",
    "    path_to_video = df_filename['path_to_video'].unique()[0]\n",
    "    path_to_dlc_coords = df_filename['dlc_result_file'].unique()[0]\n",
    "    test_dict = get_rois_per_video(path_to_video, path_to_dlc_coords)\n",
    "    df_grouped = df_filename.groupby('hdbscan_clusters')\n",
    "\n",
    "    for i, clus in (enumerate(clusters_fn)):\n",
    "\n",
    "        df = df_grouped.get_group(clus)\n",
    "        if len(df.index) > 5:\n",
    "            df_samples = df.sample(5)\n",
    "        else:\n",
    "            df_samples = df\n",
    "\n",
    "        frames = list(df_samples['frame'])\n",
    "        for f in frames:\n",
    "            mega_dict[clus].append(test_dict[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e80eaa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(mega_dict[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d0975",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 10, figsize=(35, 50))\n",
    "for i, clus in enumerate(mega_dict.keys()):\n",
    "    list_images_clus = mega_dict[clus]\n",
    "    if len(list_images_clus) >= 10:\n",
    "        sample_images = random.sample(list_images_clus, 10)\n",
    "    else:\n",
    "        sample_images = list_images_clus\n",
    "    \n",
    "    for j, img in enumerate(sample_images):\n",
    "        axes[i][j].imshow(img)\n",
    "        if j == 0:\n",
    "            axes[i][j].set_ylabel(f'Cluster {clus}')   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d918da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
