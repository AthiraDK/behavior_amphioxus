{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26353c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_helper import DLC_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1346b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import *\n",
    "from features_speed import *\n",
    "from preprocess_dlc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a532954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_utils import find_square_bounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeacb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104e2b8",
   "metadata": {},
   "source": [
    " # Import the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ab1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_control = pd.read_hdf('../../results/UMAP_HDBSCANclustering_withWV_31072023_1135.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_control.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db05cc1",
   "metadata": {},
   "source": [
    "# Plot the UMAP & clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_control = list(df_results_control['hdbscan_wv_scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = df_results_control.filter(like = 'umap_raw').values\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb08157",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clusters = {f'cluster_{i}':np.sum(clusters_control==i) for i in list(np.unique(clusters_control))}\n",
    "dict_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_pal = sns.color_palette('tab10', 10)\n",
    "c_dict = {i: c_pal[i+1] for i in np.unique(clusters_control)}\n",
    "labels_c = [c_dict[lab] for lab in clusters_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f4626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "axes= axes.ravel()\n",
    "axes[0].scatter(embedding[:, 0],embedding[:, 1], s=0.2)\n",
    "axes[1].scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1], c=labels_c, s=1)\n",
    "\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in c_dict.values()]\n",
    "plt.legend(markers, c_dict.keys(), numpoints=1)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    \n",
    "# fig.savefig('../../results/umap_clustered.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_control.groupby('hdbscan_wv_scaled').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb464a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_control.groupby('hdbscan_wv_scaled').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045453de",
   "metadata": {},
   "source": [
    "# Check feature statistics in each groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_control.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_feats = df_results_control.groupby('hdbscan_wv_scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e29762",
   "metadata": {},
   "source": [
    "## Speed_MOUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539671be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    sns.histplot(data=group, x='speed_MOUTH', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    sns.boxplot(data=group, y='speed_MOUTH', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25388214",
   "metadata": {},
   "source": [
    "## Speed_V(entral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7104e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    group['mean_speeds_ventral'] = group.filter(like='speed_V').mean(axis=1)\n",
    "    sns.histplot(data=group, x='mean_speeds_ventral', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0855593",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    group['mean_speeds_ventral'] = group.filter(like='speed_V').mean(axis=1)\n",
    "    sns.boxplot(data=group, y='mean_speeds_ventral', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f75897",
   "metadata": {},
   "source": [
    "## Speed_D(orsal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    group =  group.fillna(value=-1)\n",
    "    group['mean_speeds_dorsal'] = group.filter(like='speed_D').mean(axis=1)\n",
    "    sns.histplot(data=group, x='mean_speeds_dorsal', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd072247",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    group =  group.fillna(value=-1)\n",
    "    group['mean_speeds_dorsal'] = group.filter(like='speed_D').mean(axis=1)\n",
    "    sns.boxplot(data=group, y='mean_speeds_dorsal', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274296c7",
   "metadata": {},
   "source": [
    "## Speed_NT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    sns.histplot(data=group, x='speed_NT', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    sns.boxplot(data=group, y='speed_NT', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3a879",
   "metadata": {},
   "source": [
    "## Curvatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba859aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    group =  group.fillna(value=-1)\n",
    "    group['mean_curv'] = group.filter(like='curv').abs().mean(axis=1)\n",
    "    sns.histplot(data=group, x='mean_curv', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d00908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    group =  group.fillna(value=-1)\n",
    "    group['mean_curv'] = group.filter(like='curv').abs().mean(axis=1)\n",
    "    sns.boxplot(data=group, y='mean_curv', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1e738",
   "metadata": {},
   "source": [
    "## Quirkiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    sns.histplot(data=group, x='quirkiness', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93856fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize = (32,8), sharex=True, sharey=True)\n",
    "for i, (cluster, group) in enumerate(grouped_feats):\n",
    "    sns.boxplot(data=group, y='quirkiness', ax=axes[i])\n",
    "    axes[i].set_title(f'Cluster {cluster}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed7aa6",
   "metadata": {},
   "source": [
    "# Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbfb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(df_results_control.filename.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "wid_fn = widgets.SelectMultiple(\n",
    "    options=filenames,\n",
    "    value=filenames[:2],\n",
    "    rows=15,\n",
    "    description='Filename',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67162c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual\n",
    "def plot_trajectory(fns=wid_fn):\n",
    "    \n",
    "    for fn in fns:\n",
    "        df_filename = df_results_control[df_results_control['filename']== fn]\n",
    "        path_to_video = df_filename['path_to_video'].unique()[0]\n",
    "        print(path_to_video)\n",
    "    \n",
    "    n_cols = len(fns)\n",
    "    fig, axes = plt.subplots(1,n_cols,figsize=(n_cols*8,8), sharex=True, sharey=True)\n",
    "    \n",
    "    \n",
    "    for i, fn in enumerate(fns):\n",
    "        \n",
    "        df_result_fn = df_results_control[df_results_control['filename'] == fn]\n",
    "        \n",
    "        # data from DLC \n",
    "        \n",
    "        dlc_path = df_result_fn['dlc_result_file'].unique()[0]\n",
    "        dlc_folder, dlc_filename = os.path.split(dlc_path)\n",
    "        dlc_obj = DLC_tracking(dlc_filename, dlc_folder)\n",
    "        \n",
    "        # data from clustering\n",
    "        df_cluster = pd.merge(dlc_obj.df_data, df_result_fn, on='frame')\n",
    "        hue = [c_dict[clus] for clus in df_cluster['hdbscan_wv_scaled']]\n",
    "        \n",
    "        xy = df_cluster[['NT_x', 'NT_y']].values\n",
    "        axes[i].scatter(xy[:,0], xy[:,1], c=hue, s=2)\n",
    "#         markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in c_dict.values()]\n",
    "#         axes[i].legend(markers, c_dict.keys(), numpoints=1)\n",
    "        axes[i].set_aspect('equal')\n",
    "        axes[i].set_title(fn)\n",
    "        \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual\n",
    "def plot_trajectory_line(fns=wid_fn):\n",
    "    \n",
    "    from matplotlib.collections import LineCollection\n",
    "    \n",
    "    for fn in fns:\n",
    "        df_filename = df_results_control[df_results_control['filename']== fn]\n",
    "        path_to_video = df_filename['path_to_video'].unique()[0]\n",
    "        print(path_to_video)\n",
    "    \n",
    "    n_cols = len(fns)\n",
    "    fig, axes = plt.subplots(1,n_cols,figsize=(n_cols*8,8), sharex=True, sharey=True)\n",
    "    \n",
    "    \n",
    "    for i, fn in enumerate(fns):\n",
    "        \n",
    "        df_result_fn = df_results_control[df_results_control['filename'] == fn]\n",
    "        \n",
    "        # data from DLC \n",
    "        \n",
    "        dlc_path = df_result_fn['dlc_result_file'].unique()[0]\n",
    "        dlc_folder, dlc_filename = os.path.split(dlc_path)\n",
    "        dlc_obj = DLC_tracking(dlc_filename, dlc_folder)\n",
    "        \n",
    "        # data from clustering\n",
    "        df_cluster = pd.merge(dlc_obj.df_data, df_result_fn, on='frame')\n",
    "        hue = [c_dict[clus] for clus in df_cluster['hdbscan_wv_scaled']]\n",
    "        \n",
    "        xy = df_cluster[['NT_x', 'NT_y']].values\n",
    "        xy = xy.reshape(-1, 1, 2)\n",
    "        segments = np.hstack([xy[:-1], xy[1:]])\n",
    "\n",
    "        coll = LineCollection(segments, colors=hue)\n",
    "#         coll.set_array(np.random.random(xy.shape[0]))\n",
    "\n",
    "        axes[i].add_collection(coll)\n",
    "        axes[i].autoscale_view()\n",
    "        axes[i].set_title(fn)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c0f7c",
   "metadata": {},
   "source": [
    "# Temporal properties : transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aac63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fe687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43f36d",
   "metadata": {},
   "source": [
    "## Time in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5257a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files_grouped = df_results_control.groupby('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_usage = []\n",
    "for name, df_file in df_files_grouped:\n",
    "    clusters_file = df_file['hdbscan_wv_scaled'].values\n",
    "    test_count = Counter(clusters_file)\n",
    "    dict_cluster_usage = {}\n",
    "    dict_cluster_usage['filename'] = name\n",
    "    for k in sorted(test_count.keys()):\n",
    "        dict_cluster_usage[f'cluster_{k}_frames'] = test_count[k]\n",
    "    cluster_usage.append(dict_cluster_usage)\n",
    "\n",
    "df_cluster_usage  = pd.DataFrame(cluster_usage)      \n",
    "df_cluster_usage.fillna(0, inplace=True)\n",
    "df_cluster_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_usage['acclimitization'] = df_cluster_usage['filename'].apply(lambda x: 1 if ((x.split('_')[3]=='15m0s')|(x.split('_')[3]=='15m3s')) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62098ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_usage.groupby('acclimitization').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cluster_usage.groupby('acclimitization').sum()\n",
    "res = df.div(df.sum(axis=1), axis=0)\n",
    "res.mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe09ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.mul(100).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5978e87",
   "metadata": {},
   "source": [
    "## Transition counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2fc5ef",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47fd35",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s = \"1110002223344555551111\"\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "groups = groupby(s)\n",
    "result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a41d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clusters_file = df_file['hdbscan_wv_scaled'].values\n",
    "start_list = [x for x in clusters_file[:-1]]\n",
    "stop_list = [x for x in clusters_file[1:]]\n",
    "\n",
    "trans_dict = {'start': start_list, 'stop': stop_list}\n",
    "trans_df = pd.DataFrame(trans_dict)\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174093d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trans_df.groupby(['start', 'stop']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b352645",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transition_counts_control = trans_df.groupby(['start', 'stop']).size().reset_index(name='counts')\n",
    "trans_mat_counts = pd.pivot_table(transition_counts_control, values='counts', index=['start'],\n",
    "                columns=['stop'])\n",
    "trans_mat_counts = trans_mat_counts.fillna(0)\n",
    "trans_mat_probs = trans_mat_counts.div(trans_mat_counts.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd0f23",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trans_mat_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e93d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(trans_mat_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a5ec6",
   "metadata": {},
   "source": [
    "### transition matrix - all, acclimitization, non-acclimitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8306aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_usage_acc = []\n",
    "cluster_usage_non_acc = []\n",
    "for name, df_file in df_files_grouped:\n",
    "    clusters_file = df_file['hdbscan_wv_scaled'].values\n",
    "    start_list = [x for x in clusters_file[:-1]]\n",
    "    stop_list = [x for x in clusters_file[1:]]\n",
    "\n",
    "    trans_dict = {'start': start_list, 'stop': stop_list}\n",
    "    trans_df = pd.DataFrame(trans_dict)\n",
    "    \n",
    "    exp_duration = name.split('_')[3]\n",
    "    if  (exp_duration == '15m0s')|(exp_duration == '15m3s'):\n",
    "        cluster_usage_acc.append(trans_df)\n",
    "    elif  (exp_duration == '5m0s')|(exp_duration == '5m3s'):\n",
    "        cluster_usage_non_acc.append(trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_df_acc = pd.concat(cluster_usage_acc)\n",
    "transition_df_non_acc = pd.concat(cluster_usage_non_acc)\n",
    "transition_df = pd.concat(cluster_usage_acc + cluster_usage_non_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6131824",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts_acc = transition_df_acc.groupby(['start', 'stop']).size().reset_index(name='counts')\n",
    "transition_counts_non_acc = transition_df_non_acc.groupby(['start', 'stop']).size().reset_index(name='counts')\n",
    "transition_counts = transition_df.groupby(['start', 'stop']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat_counts_acc = pd.pivot_table(transition_counts_acc, values='counts', index=['start'],\n",
    "                columns=['stop'])\n",
    "trans_mat_counts_non_acc = pd.pivot_table(transition_counts_non_acc, values='counts', index=['start'],\n",
    "                columns=['stop'])\n",
    "trans_mat_counts = pd.pivot_table(transition_counts, values='counts', index=['start'],\n",
    "                columns=['stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4515fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat_counts_acc = trans_mat_counts_acc.fillna(0)\n",
    "trans_mat_counts_non_acc = trans_mat_counts_non_acc.fillna(0)\n",
    "trans_mat_counts = trans_mat_counts.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec81212",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat_probs_acc = trans_mat_counts_acc.div(trans_mat_counts_acc.sum(axis=1))\n",
    "trans_mat_probs_non_acc = trans_mat_counts_non_acc.div(trans_mat_counts_non_acc.sum(axis=1))\n",
    "trans_mat_probs = trans_mat_counts.div(trans_mat_counts.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a40074",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8441b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(trans_mat_probs_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4757bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(trans_mat_probs_non_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(trans_mat_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe85ac9",
   "metadata": {},
   "source": [
    "## Lengths of cluster stretches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aadb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cluster_motifs_df(fn, df_file):   \n",
    "    \n",
    "    clusters_file = df_file['hdbscan_wv_scaled'].values\n",
    "    frames = df_file['frame'].values\n",
    "    \n",
    "    df_motif = []\n",
    "    \n",
    "    \n",
    "    for state in np.unique(clusters_file):\n",
    "\n",
    "        clus = {}\n",
    "        clus_inds = [ind for ind, val in zip(frames, clusters_file) if val == state]\n",
    "        clus_inds_nested = [list(map(itemgetter(1), g)) for k, g in groupby(enumerate(clus_inds), lambda x: x[0]-x[1])]\n",
    "        clus['start'] = [x[0] for x in clus_inds_nested]\n",
    "        clus['stop'] = [x[-1] for x in clus_inds_nested]\n",
    "        clus['duration'] = [x[-1]-x[0] for x in clus_inds_nested]\n",
    "        clus['cluster'] = [state for x in clus_inds_nested]\n",
    "        clus['filename'] = [fn for x in clus_inds_nested]\n",
    "        df_clus = pd.DataFrame(clus)\n",
    "\n",
    "        df_motif.append(df_clus)\n",
    "    df_motif = pd.concat(df_motif) \n",
    "    return df_motif\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_motifs_all = Parallel(n_jobs=40, verbose = 5)(delayed(make_cluster_motifs_df)(fn, df_fn) \n",
    "                                                for fn, df_fn in df_files_grouped)\n",
    "df_motifs_combined = pd.concat(df_motifs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d630044",
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_groups  = df_motifs_combined.groupby('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize=(20, 8), sharey=True)\n",
    "for i, (clus, motif_g) in enumerate(motif_groups):\n",
    "    sns.boxplot(data = motif_g, x='cluster', y='duration', ax =axes[clus+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea683447",
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_groups.agg({'duration':[min, max, np.mean]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30115a8",
   "metadata": {},
   "source": [
    "# Path complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f47c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_M(X, Y, window):\n",
    "    \"\"\"returns normalized embedding matrix M for columns X and Y with the specified window size.\n",
    "    This matrix can be passed to the get_H function to compute the complexity value\"\"\"\n",
    "    Mx = np.array(X[:window]) #initialize first row of Mx\n",
    "    My = np.array(Y[:window]) #initialize first row of My\n",
    "    for ii in range(1, len(X)-window): #skip first entry since we already have that in M\n",
    "        Mx = np.vstack([Mx, X[ii:ii+window]]) #add new vector to Mx\n",
    "        My = np.vstack([My, Y[ii:ii+window]]) #add new vector to My\n",
    "    \n",
    "    \n",
    "    Mx = StandardScaler().fit_transform(Mx)\n",
    "    My = StandardScaler().fit_transform(My)\n",
    "    \n",
    "#     cols = Mx.shape[1] #get number of columns from array object\n",
    "#     for ii in range(cols): #normalize per column:\n",
    "#         Mx[:,ii] = Mx[:,ii] - np.nanmean(Mx[:,ii])\n",
    "#         My[:,ii] = My[:,ii] - np.nanmean(My[:,ii])\n",
    "\n",
    "    \n",
    "    M = np.dstack([Mx,My]) #stack the arrays Mx and My   \n",
    "    return M #return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H(M):\n",
    "    \"\"\"Performs singular value decomposition on M, and uses the diagonal matrix S\n",
    "    to calculate complexity value H as the entropy in the distribution of components of S\n",
    "    I advise you to read Herbert-Read (2017) on escape path complexity\"\"\"\n",
    "    U,S,V = np.linalg.svd(M) # do singular value decomposition\n",
    "    hats_array = [s/np.sum(s) for s in S] #make hats array\n",
    "    local_H = [-np.sum(s*np.log2(s)) for s in hats_array]\n",
    "#     H = -np.sum([s*np.log2(s) for s in hats_array]) #calculate H\n",
    "    H = -np.sum(hats_array * np.log2(hats_array))\n",
    "    return local_H,H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual\n",
    "def plot_path_complexity(fns=wid_fn):\n",
    "    \n",
    "    from matplotlib.collections import LineCollection\n",
    "    \n",
    "    for fn in fns:\n",
    "        df_filename = df_results_control[df_results_control['filename']== fn]\n",
    "        path_to_video = df_filename['path_to_video'].unique()[0]\n",
    "        \n",
    "    \n",
    "    n_cols = len(fns)\n",
    "    fig, axes = plt.subplots(1,n_cols,figsize=(n_cols*12,12), sharex=True, sharey=True)\n",
    "    \n",
    "    complexity = []\n",
    "    for i, fn in enumerate(fns):\n",
    "        \n",
    "        print(fn)\n",
    "        \n",
    "        df_result_fn = df_results_control[df_results_control['filename'] == fn]\n",
    "        \n",
    "        # data from DLC \n",
    "        \n",
    "        dlc_path = df_result_fn['dlc_result_file'].unique()[0]\n",
    "        dlc_folder, dlc_filename = os.path.split(dlc_path)\n",
    "        dlc_obj = DLC_tracking(dlc_filename, dlc_folder)\n",
    "        \n",
    "        # data from clustering\n",
    "        df_cluster = pd.merge(dlc_obj.df_data, df_result_fn, on='frame')\n",
    "        hue = [c_dict[clus] for clus in df_cluster['hdbscan_wv_scaled']]\n",
    "        \n",
    "        framerate = 30\n",
    "        window = framerate * 3\n",
    "        \n",
    "        df_cluster['NT_x'] = df_cluster['NT_x'] \n",
    "        df_cluster['NT_y'] = df_cluster['NT_y'] \n",
    "        \n",
    "        \n",
    "        df_xy = df_cluster[['NT_x', 'NT_y']] \n",
    "        df_xy = df_xy.dropna()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            M = obtain_M(df_xy['NT_x'], df_xy['NT_y'], window = window)\n",
    "\n",
    "            lH,H = get_H(M)\n",
    "\n",
    "            df_xy['lH'] = np.hstack((lH, np.array([np.nan]*window)))\n",
    "            \n",
    "            xy = df_xy.values\n",
    "            \n",
    "            axes[i].scatter(xy[:,0], xy[:,1], c=df_xy['lH'], s=2, cmap='jet')\n",
    "            \n",
    "            print(df_xy.lH.mean(), H)\n",
    "\n",
    "            complexity.append(df_xy.lH.median())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e, fn)\n",
    "    \n",
    "        \n",
    "    return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_path_complexity(filename):\n",
    "    \n",
    "    df_result_fn = df_results_control[df_results_control['filename'] == filename]\n",
    "        \n",
    "    # data from DLC \n",
    "\n",
    "    dlc_path = df_result_fn['dlc_result_file'].unique()[0]\n",
    "    dlc_folder, dlc_filename = os.path.split(dlc_path)\n",
    "    dlc_obj = DLC_tracking(dlc_filename, dlc_folder)\n",
    "  \n",
    "    \n",
    "    # Interpolate missing datapoints (dorsal)\n",
    "    df_dorsal = dlc_obj.df_data.filter(regex='^(NT_|TT_|D).*(x|y)$')\n",
    "    df_dorsal_filt = df_dorsal[df_dorsal.isna().sum(axis=1) < 5]\n",
    "    df_dorsal_x = df_dorsal_filt.filter(like='_x')\n",
    "    df_dorsal_y = df_dorsal_filt.filter(like='_y')\n",
    "    df_dorsal_interp_x = interpol_spatial(df_dorsal_x)\n",
    "    df_dorsal_interp_y = interpol_spatial(df_dorsal_y)\n",
    "    df_dorsal_x_fin = interpol_temporal(df_dorsal_interp_x)\n",
    "    df_dorsal_y_fin = interpol_temporal(df_dorsal_interp_y)\n",
    "    dlc_obj.df_data.loc[df_dorsal_filt.index,'NT_x_interp'] = df_dorsal_x_fin['NT_x']\n",
    "    dlc_obj.df_data.loc[df_dorsal_filt.index,'NT_y_interp'] = df_dorsal_y_fin['NT_y']\n",
    "    \n",
    "    # data from clustering  # need not do this !\n",
    "    df_cluster = pd.merge(dlc_obj.df_data, df_result_fn, on='frame')\n",
    "    hue = [c_dict[clus] for clus in df_cluster['hdbscan_wv_scaled']]\n",
    "\n",
    "\n",
    "    framerate = 30\n",
    "    window = framerate \n",
    "\n",
    "    df_xy = df_cluster[['filename','frame','NT_x_interp', 'NT_y_interp', 'NT_x', 'NT_y']] \n",
    "#     df_xy = df_xy.dropna(how='any')\n",
    "        \n",
    "    try:\n",
    "\n",
    "        M = obtain_M(df_xy['NT_x_interp'], df_xy['NT_y_interp'], window = window)\n",
    "\n",
    "        lH,H = get_H(M)\n",
    "        \n",
    "        df_xy['lH'] = np.hstack((np.array([np.nan]*(window//2)), lH, np.array([np.nan]*(window - (window//2)))))\n",
    "        return df_xy\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lH = calc_path_complexity(filenames[-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lH['lH'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a02e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lH_all[-8].lH.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lH_all = Parallel(n_jobs=40, verbose = 5)(delayed(calc_path_complexity)(fn) \n",
    "                                                for fn in filenames)\n",
    "df_lH_combined = pd.concat(df_lH_all)\n",
    "df_lH_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_merged_complexity = pd.merge(df_lH_combined, df_results_control, on=['filename','frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb674495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_merged_complexity.groupby('hdbscan_wv_scaled').agg({'lH':np.nanmean})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f0adc",
   "metadata": {},
   "source": [
    "### stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f77957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal, mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd74952",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clus_lH = {}\n",
    "for name, group in df_results_merged_complexity.groupby('hdbscan_wv_scaled'):\n",
    "    dict_clus_lH[name] = group['lH'].dropna().values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a91f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "kruskal(arr_clus_lH[1],arr_clus_lH[2],arr_clus_lH[3],arr_clus_lH[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3eaecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mwu_stat = pd.DataFrame(index=[f'clus_{i}' for i in dict_clus_lH.keys()], columns=[f'clus_{i}' for i in dict_clus_lH.keys()])\n",
    "df_mwu_pval = pd.DataFrame(index=[f'clus_{i}' for i in dict_clus_lH.keys()], columns=[f'clus_{i}' for i in dict_clus_lH.keys()])\n",
    "for clus1, clus2 in itertools.product(dict_clus_lH.keys(),dict_clus_lH.keys()):\n",
    "    mwu_results = mannwhitneyu(dict_clus_lH[clus1],dict_clus_lH[clus2])\n",
    "    df_mwu_pval.loc[f'clus_{clus1}',f'clus_{clus2}'] = mwu_results[1]\n",
    "    df_mwu_stat.loc[f'clus_{clus1}',f'clus_{clus2}'] = mwu_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_merged_complexity['acclimitization'] = df_results_merged_complexity['filename'].apply(lambda x: 1 if ((x.split('_')[3]=='15m0s')|(x.split('_')[3]=='15m3s')) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_merged_complexity.groupby(['acclimitization']).agg({'lH':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf98aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_merged_complexity.groupby(['acclimitization','hdbscan_wv_scaled']).agg({'lH':np.nanmean})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7256bd",
   "metadata": {},
   "source": [
    "## what's cluster 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3dea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_merged_complexity.groupby('hdbscan_wv_scaled').get_group(3).groupby(['filename']).nunique('frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(9,9))\n",
    "axes = axes.ravel()\n",
    "for i, (name, data) in enumerate(df_results_merged_complexity.groupby('hdbscan_wv_scaled').get_group(6).groupby(['filename'])):\n",
    "    if i >= 9:\n",
    "        break\n",
    "    else:\n",
    "        print(name)\n",
    "        print(data['frame'])\n",
    "#         data.dropna(inplace=True)\n",
    "        \n",
    "        res, last = [[]], None\n",
    "        for x in list(data.index):\n",
    "            if last is None or abs(last - x) < 2:\n",
    "                res[-1].append(x)\n",
    "            else:\n",
    "                res.append([x])\n",
    "            last = x\n",
    "#         print([len(r) for r in res])\n",
    "        for r in res:\n",
    "            if (len(r) > 2):\n",
    "                axes[i].plot(data.loc[r[0]:r[-1]]['NT_x'], data.loc[r[0]:r[-1]]['NT_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9a874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b544381",
   "metadata": {},
   "source": [
    "## Test path complexity calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091866bb",
   "metadata": {},
   "source": [
    "### Straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bfb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 30\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "y = x * 5\n",
    "\n",
    "M = obtain_M(x, y, window = window)\n",
    "\n",
    "lH,H = get_H(M)\n",
    "\n",
    "lH_padded = np.hstack((lH, np.array([np.nan]*window)))\n",
    "\n",
    "np.mean(lH_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac134cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48bc9c7",
   "metadata": {},
   "source": [
    "### Random walk path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f57fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up steps for simulating 2D\n",
    "dims = 2\n",
    "step_n = 1000\n",
    "step_set = [-1, 0, 1]\n",
    "origin = np.zeros((1,dims))\n",
    "#Simulate steps in 2D\n",
    "step_shape = (step_n,dims)\n",
    "steps = np.random.choice(a=step_set, size=step_shape)\n",
    "path = np.concatenate([origin, steps]).cumsum(0)\n",
    "\n",
    "path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(path[:,0], path[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2620daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = obtain_M(path[:,0], path[:,1], window = window)\n",
    "\n",
    "lH,H = get_H(M)\n",
    "\n",
    "lH_padded = np.hstack((lH, np.array([np.nan]*window)))\n",
    "\n",
    "np.mean(lH_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "pi = math.pi\n",
    "\n",
    "def PointsInCircum(r,n=1000):\n",
    "    x = [math.cos(2*pi/n*x)*r for x in range(0,n+1)]\n",
    "    y = [math.sin(2*pi/n*x)*r  for x in range(0,n+1)]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cir = PointsInCircum(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(path_cir[0], path_cir[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = obtain_M(path_cir[0], path_cir[1], window = window)\n",
    "\n",
    "lH,H = get_H(M)\n",
    "\n",
    "lH_padded = np.hstack((lH, np.array([np.nan]*window)))\n",
    "\n",
    "np.mean(lH_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5a0d0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Discrete HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c5bf9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM # Change to CategoricalHMM if using version 0.28 or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9983643",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_files_grouped = df_results_control.groupby('filename')\n",
    "\n",
    "X = []\n",
    "lengths = []\n",
    "\n",
    "for name, group in df_files_grouped:\n",
    "    group['hdbscan_wv_scaled_plus'] = group['hdbscan_wv_scaled'].apply(lambda x: x+1)\n",
    "    x_cluster = group['hdbscan_wv_scaled_plus'].values\n",
    "    X.append(x_cluster)\n",
    "    lengths.append(len(group.index))\n",
    "X = np.concatenate(X).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb4d50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104844e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hmm_discrete = MultinomialHMM(n_components=8)\n",
    "hmm_discrete.fit(X, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8aad6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if hmm_discrete.monitor_.converged:\n",
    "    print(\"Model converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799d391",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(hmm_discrete.emissionprob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964b5c9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(hmm_discrete.transmat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b4f53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_predicted = hmm_discrete.predict(X, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcaffb7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_results_control['hmm_discrete'] = X_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6934106",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@interact_manual\n",
    "def plot_hmm_line(fns=wid_fn):\n",
    "    \n",
    "    from matplotlib.collections import LineCollection\n",
    "    \n",
    "    for fn in fns:\n",
    "        df_filename = df_results_control[df_results_control['filename']== fn]\n",
    "        path_to_video = df_filename['path_to_video'].unique()[0]\n",
    "        print(path_to_video)\n",
    "    \n",
    "    n_cols = len(fns)\n",
    "    fig, axes = plt.subplots(1,n_cols,figsize=(n_cols*8,8), sharex=True, sharey=True)\n",
    "    \n",
    "    \n",
    "    for i, fn in enumerate(fns):\n",
    "        \n",
    "        df_result_fn = df_results_control[df_results_control['filename'] == fn]\n",
    "        \n",
    "        # data from DLC \n",
    "        \n",
    "        dlc_path = df_result_fn['dlc_result_file'].unique()[0]\n",
    "        dlc_folder, dlc_filename = os.path.split(dlc_path)\n",
    "        dlc_obj = DLC_tracking(dlc_filename, dlc_folder)\n",
    "        \n",
    "        # data from clustering\n",
    "        df_cluster = pd.merge(dlc_obj.df_data, df_result_fn, on='frame')\n",
    "        hue = [c_dict[clus-1] for clus in df_cluster['hmm_discrete']]\n",
    "        \n",
    "        xy = df_cluster[['NT_x', 'NT_y']].values\n",
    "        xy = xy.reshape(-1, 1, 2)\n",
    "        segments = np.hstack([xy[:-1], xy[1:]])\n",
    "\n",
    "        coll = LineCollection(segments, colors=hue)\n",
    "#         coll.set_array(np.random.random(xy.shape[0]))\n",
    "\n",
    "        axes[i].add_collection(coll)\n",
    "        axes[i].autoscale_view()\n",
    "        axes[i].set_title(fn)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c86d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
